/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.
  warnings.warn(
Epoch 1/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 1.3528 - mae: 0.4860 - val_loss: 0.8271 - val_mae: 0.1247 - learning_rate: 5.0000e-04
Epoch 2/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.8903 - mae: 0.2186 - val_loss: 0.7789 - val_mae: 0.1070 - learning_rate: 5.0000e-04
Epoch 3/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 19s 13ms/step - loss: 0.8610 - mae: 0.1812 - val_loss: 0.7544 - val_mae: 0.1010 - learning_rate: 5.0000e-04
Epoch 4/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 21s 14ms/step - loss: 0.8400 - mae: 0.1684 - val_loss: 0.7506 - val_mae: 0.1000 - learning_rate: 5.0000e-04
Epoch 5/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 13ms/step - loss: 0.8165 - mae: 0.1689 - val_loss: 0.7396 - val_mae: 0.1019 - learning_rate: 5.0000e-04
Epoch 6/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.8000 - mae: 0.1705 - val_loss: 0.7225 - val_mae: 0.1014 - learning_rate: 5.0000e-04
Epoch 7/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7647 - mae: 0.1739 - val_loss: 0.7125 - val_mae: 0.1031 - learning_rate: 5.0000e-04
Epoch 8/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7776 - mae: 0.1774 - val_loss: 0.7122 - val_mae: 0.1044 - learning_rate: 5.0000e-04
Epoch 9/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7884 - mae: 0.1821 - val_loss: 0.6948 - val_mae: 0.1050 - learning_rate: 5.0000e-04
Epoch 10/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.7855 - mae: 0.1845 - val_loss: 0.6857 - val_mae: 0.1059 - learning_rate: 5.0000e-04
Epoch 11/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 19s 13ms/step - loss: 0.7768 - mae: 0.1863 - val_loss: 0.6877 - val_mae: 0.1076 - learning_rate: 5.0000e-04
Epoch 12/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 21s 14ms/step - loss: 0.7883 - mae: 0.1890 - val_loss: 0.6771 - val_mae: 0.1083 - learning_rate: 5.0000e-04
Epoch 13/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7607 - mae: 0.1897 - val_loss: 0.6731 - val_mae: 0.1100 - learning_rate: 5.0000e-04
Epoch 14/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.7606 - mae: 0.1925 - val_loss: 0.6686 - val_mae: 0.1079 - learning_rate: 5.0000e-04
Epoch 15/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7638 - mae: 0.1928 - val_loss: 0.6727 - val_mae: 0.1090 - learning_rate: 5.0000e-04
Epoch 16/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7492 - mae: 0.1940 - val_loss: 0.6743 - val_mae: 0.1106 - learning_rate: 5.0000e-04
Epoch 17/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.7679 - mae: 0.1948 - val_loss: 0.6654 - val_mae: 0.1079 - learning_rate: 5.0000e-04
Epoch 18/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 13ms/step - loss: 0.7815 - mae: 0.1968 - val_loss: 0.6648 - val_mae: 0.1094 - learning_rate: 5.0000e-04
Epoch 19/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 21s 14ms/step - loss: 0.7638 - mae: 0.1986 - val_loss: 0.6618 - val_mae: 0.1099 - learning_rate: 5.0000e-04
Epoch 20/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7611 - mae: 0.1980 - val_loss: 0.6648 - val_mae: 0.1112 - learning_rate: 5.0000e-04
Epoch 21/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 24s 18ms/step - loss: 0.7595 - mae: 0.1988 - val_loss: 0.6622 - val_mae: 0.1107 - learning_rate: 5.0000e-04
Epoch 22/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7608 - mae: 0.1995 - val_loss: 0.6788 - val_mae: 0.1128 - learning_rate: 5.0000e-04
Epoch 23/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.7402 - mae: 0.2002 - val_loss: 0.6590 - val_mae: 0.1099 - learning_rate: 5.0000e-04
Epoch 24/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 21s 14ms/step - loss: 0.7687 - mae: 0.2020 - val_loss: 0.6516 - val_mae: 0.1097 - learning_rate: 5.0000e-04
Epoch 25/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7446 - mae: 0.2006 - val_loss: 0.6531 - val_mae: 0.1094 - learning_rate: 5.0000e-04
Epoch 26/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.7361 - mae: 0.2021 - val_loss: 0.6491 - val_mae: 0.1102 - learning_rate: 5.0000e-04
Epoch 27/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 19s 13ms/step - loss: 0.7491 - mae: 0.2016 - val_loss: 0.6441 - val_mae: 0.1101 - learning_rate: 5.0000e-04
Epoch 28/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 13s 13ms/step - loss: 0.7571 - mae: 0.2013 - val_loss: 0.6611 - val_mae: 0.1134 - learning_rate: 5.0000e-04
Epoch 29/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 22s 14ms/step - loss: 0.7517 - mae: 0.2043 - val_loss: 0.6550 - val_mae: 0.1129 - learning_rate: 5.0000e-04
Epoch 30/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7647 - mae: 0.2037 - val_loss: 0.6549 - val_mae: 0.1112 - learning_rate: 5.0000e-04
Epoch 31/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7402 - mae: 0.2036 - val_loss: 0.6586 - val_mae: 0.1124 - learning_rate: 5.0000e-04
Epoch 32/100
1028/1030 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - loss: 0.7407 - mae: 0.2050
Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 13ms/step - loss: 0.7408 - mae: 0.2050 - val_loss: 0.6463 - val_mae: 0.1123 - learning_rate: 5.0000e-04
Epoch 33/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 22s 14ms/step - loss: 0.7734 - mae: 0.2040 - val_loss: 0.6445 - val_mae: 0.1087 - learning_rate: 2.5000e-04
Epoch 34/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7554 - mae: 0.2038 - val_loss: 0.6379 - val_mae: 0.1089 - learning_rate: 2.5000e-04
Epoch 35/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7834 - mae: 0.2039 - val_loss: 0.6526 - val_mae: 0.1130 - learning_rate: 2.5000e-04
Epoch 36/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 15ms/step - loss: 0.7425 - mae: 0.2046 - val_loss: 0.6446 - val_mae: 0.1107 - learning_rate: 2.5000e-04
Epoch 37/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.7272 - mae: 0.2045 - val_loss: 0.6471 - val_mae: 0.1118 - learning_rate: 2.5000e-04
Epoch 38/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.7216 - mae: 0.2049 - val_loss: 0.6471 - val_mae: 0.1116 - learning_rate: 2.5000e-04
Epoch 39/100
1027/1030 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - loss: 0.7508 - mae: 0.2052
Epoch 39: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7507 - mae: 0.2052 - val_loss: 0.6444 - val_mae: 0.1110 - learning_rate: 2.5000e-04
Epoch 40/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7453 - mae: 0.2055 - val_loss: 0.6404 - val_mae: 0.1095 - learning_rate: 1.2500e-04
Epoch 41/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 21s 14ms/step - loss: 0.7532 - mae: 0.2062 - val_loss: 0.6346 - val_mae: 0.1095 - learning_rate: 1.2500e-04
Epoch 42/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 13ms/step - loss: 0.7474 - mae: 0.2054 - val_loss: 0.6413 - val_mae: 0.1104 - learning_rate: 1.2500e-04
Epoch 43/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 13s 13ms/step - loss: 0.7420 - mae: 0.2054 - val_loss: 0.6353 - val_mae: 0.1093 - learning_rate: 1.2500e-04
Epoch 44/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 13ms/step - loss: 0.7578 - mae: 0.2062 - val_loss: 0.6361 - val_mae: 0.1086 - learning_rate: 1.2500e-04
Epoch 45/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 21s 14ms/step - loss: 0.7402 - mae: 0.2052 - val_loss: 0.6510 - val_mae: 0.1113 - learning_rate: 1.2500e-04
Epoch 46/100
1028/1030 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - loss: 0.7301 - mae: 0.2052
Epoch 46: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 21s 14ms/step - loss: 0.7301 - mae: 0.2052 - val_loss: 0.6452 - val_mae: 0.1100 - learning_rate: 1.2500e-04
Epoch 47/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7541 - mae: 0.2064 - val_loss: 0.6462 - val_mae: 0.1114 - learning_rate: 6.2500e-05
Epoch 48/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7473 - mae: 0.2059 - val_loss: 0.6382 - val_mae: 0.1096 - learning_rate: 6.2500e-05
Epoch 49/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.7327 - mae: 0.2058 - val_loss: 0.6345 - val_mae: 0.1091 - learning_rate: 6.2500e-05
Epoch 50/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 19s 13ms/step - loss: 0.7561 - mae: 0.2068 - val_loss: 0.6344 - val_mae: 0.1103 - learning_rate: 6.2500e-05
Epoch 51/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 13s 13ms/step - loss: 0.7390 - mae: 0.2055 - val_loss: 0.6453 - val_mae: 0.1114 - learning_rate: 6.2500e-05
Epoch 52/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 13ms/step - loss: 0.7484 - mae: 0.2056 - val_loss: 0.6419 - val_mae: 0.1098 - learning_rate: 6.2500e-05
Epoch 53/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 22s 14ms/step - loss: 0.7532 - mae: 0.2066 - val_loss: 0.6320 - val_mae: 0.1092 - learning_rate: 6.2500e-05
Epoch 54/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7665 - mae: 0.2058 - val_loss: 0.6386 - val_mae: 0.1104 - learning_rate: 6.2500e-05
Epoch 55/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 13s 13ms/step - loss: 0.7443 - mae: 0.2073 - val_loss: 0.6292 - val_mae: 0.1078 - learning_rate: 6.2500e-05
Epoch 56/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 24s 17ms/step - loss: 0.7314 - mae: 0.2063 - val_loss: 0.6288 - val_mae: 0.1084 - learning_rate: 6.2500e-05
Epoch 57/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7430 - mae: 0.2061 - val_loss: 0.6359 - val_mae: 0.1096 - learning_rate: 6.2500e-05
Epoch 58/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7157 - mae: 0.2056 - val_loss: 0.6491 - val_mae: 0.1115 - learning_rate: 6.2500e-05
Epoch 59/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7526 - mae: 0.2066 - val_loss: 0.6322 - val_mae: 0.1084 - learning_rate: 6.2500e-05
Epoch 60/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 20s 14ms/step - loss: 0.6958 - mae: 0.2051 - val_loss: 0.6450 - val_mae: 0.1109 - learning_rate: 6.2500e-05
Epoch 61/100
1026/1030 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - loss: 0.7589 - mae: 0.2071
Epoch 61: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 14ms/step - loss: 0.7588 - mae: 0.2071 - val_loss: 0.6331 - val_mae: 0.1090 - learning_rate: 6.2500e-05
Epoch 62/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 21s 15ms/step - loss: 0.7112 - mae: 0.2060 - val_loss: 0.6407 - val_mae: 0.1100 - learning_rate: 3.1250e-05
Epoch 63/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 16s 15ms/step - loss: 0.7165 - mae: 0.2064 - val_loss: 0.6282 - val_mae: 0.1077 - learning_rate: 3.1250e-05
Epoch 64/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 15s 15ms/step - loss: 0.7591 - mae: 0.2071 - val_loss: 0.6339 - val_mae: 0.1084 - learning_rate: 3.1250e-05
Epoch 65/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 19s 13ms/step - loss: 0.7440 - mae: 0.2069 - val_loss: 0.6348 - val_mae: 0.1102 - learning_rate: 3.1250e-05
Epoch 66/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7352 - mae: 0.2061 - val_loss: 0.6472 - val_mae: 0.1121 - learning_rate: 3.1250e-05
Epoch 67/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 21s 14ms/step - loss: 0.7253 - mae: 0.2064 - val_loss: 0.6397 - val_mae: 0.1097 - learning_rate: 3.1250e-05
Epoch 68/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - loss: 0.7497 - mae: 0.2062
Epoch 68: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 18s 17ms/step - loss: 0.7496 - mae: 0.2062 - val_loss: 0.6375 - val_mae: 0.1105 - learning_rate: 3.1250e-05
Epoch 69/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7073 - mae: 0.2056 - val_loss: 0.6312 - val_mae: 0.1078 - learning_rate: 1.5625e-05
Epoch 70/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 13ms/step - loss: 0.7257 - mae: 0.2063 - val_loss: 0.6283 - val_mae: 0.1075 - learning_rate: 1.5625e-05
Epoch 71/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 13ms/step - loss: 0.7555 - mae: 0.2070 - val_loss: 0.6315 - val_mae: 0.1091 - learning_rate: 1.5625e-05
Epoch 72/100
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 22s 15ms/step - loss: 0.7217 - mae: 0.2059 - val_loss: 0.6304 - val_mae: 0.1088 - learning_rate: 1.5625e-05
Epoch 73/100
1029/1030 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - loss: 0.7274 - mae: 0.2072
Epoch 73: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.
1030/1030 ━━━━━━━━━━━━━━━━━━━━ 14s 14ms/step - loss: 0.7274 - mae: 0.2072 - val_loss: 0.6431 - val_mae: 0.1107 - learning_rate: 1.5625e-05
Epoch 73: early stopping
Restoring model weights from the end of the best epoch: 63.


Total samples: 65865
Detected anomalies: 3175
Anomaly percentage: 4.82%